---
title: "Introduction to R: Statistical Models Tutorial"
author: "Dr Jeromy Anglim"
output: pdf_document
---

```{r load_project, message = FALSE, warning = FALSE}
library(ProjectTemplate); load.project()

# And create some variables
library(AER)
data("CASchools")
?CASchools
cas <- CASchools

# create new vaiables
# academic performance as the sum of reading and mathematics
# performance
cas$performance <- as.numeric(scale(cas$read) + scale(cas$math))

# student-staff ratio
cas$student_teacher_ratio <- cas$students / cas$teachers

# computers per student
cas$computer_student_ratio <- cas$computer / cas$students 

# Student size is quite skewed
hist(cas$students)
# Let's log transform it
cas$students_log <- log(cas$students)
hist(cas$students_log)

# same with average district income
cas$income_log <- log(cas$income)

dput(names(cas))
v <- list()

v$predictors <- 
    c("calworks",     # percent of students qualifying for income assistance
    "lunch",        # percent qualifying for reduced price lunch
    "expenditure",  # expenditure per student
    "english",      # percent of english learners
    "student_teacher_ratio", 
    "computer_student_ratio", 
    "students_log", 
    "income_log")
v$dv <- "performance"
v$all_variables <- c(v$predictors, v$dv)
```

# Univariate statistics
```{r}
# sample size
nrow(cas)

# Frequencies or percentages on categorical variables
table(cas$grades) # frequency counts
prop.table(table(cas$grades)) # proportions

# Descriptive statistics for continuous variables
round(psych::describe( cas[, v$all_variables]), 2)

# Descriptive statistics for categorical and numeric variables
Hmisc::describe(cas)
```


# Bivariate correlations 
```{r}
cor(cas[ , v$all_variables])
round(cor(cas[ , v$all_variables]), 2) # round to 2 decimal places
rp <- Hmisc::rcorr(as.matrix(cas[,v$all_variables])) # significance test on correlations
rp
ifelse(rp$P < .05, "*", "")

# Scatterplot matrix with correlations
pairs.panels(cas[ , v$all_variables])
```


# Regression models
```{r}
# By default, you don't get much output
lm(performance ~ expenditure + calworks + lunch, data = cas)

# You need to save the model to an object
fit <- lm(performance ~ expenditure + calworks + lunch, cas)

# this object stores the results of analyses.
# You can extract elements directly from this object
str(fit) # show the structure of the object
fit$coefficients

# But more commonly you apply a set of "methods"
summary(fit) # summary info
par(mfrow = c(2,2))
plot(fit)
anova(fit)
inf <- influence.measures(fit) # various influence and outlier measures

confint(fit) # confidence intervals on coeficients

# You can create plots yourself
# Check normality and homoscedsaticity of residuals
# plot predicted by residuals
plot(predict(fit), residuals(fit))
abline(h=0)

# standardised coefficients
library(QuantPsyc)
QuantPsyc::lm.beta(fit)
fit_standardised <- lm(scale(performance) ~ scale(expenditure) + scale(calworks) + scale(lunch), cas)
summary(fit_standardised)

# more information on regression diagnostics
# http://www.statmethods.net/stats/rdiagnostics.html
```

# Comparing regression models
```{r}
# model 1 include poverty variables
v$predictors
fit1 <- lm(performance ~ calworks + lunch + expenditure + income_log, cas)
# Model 2 adds school features
fit2 <- lm(performance ~ calworks + lunch + expenditure + income_log +
               student_teacher_ratio + students_log + 
               computer_student_ratio, cas)

summary(fit1)
summary(fit2)

# Does second model explain significantly more variance?
anova(fit1, fit2)
```

# Formula notation
```{r}
# For teaching purposes let's name the variables in a general way
x <- cas[, c("performance", "student_teacher_ratio", "students_log", "income_log")]
head(x)
names(x)  <- c("dv", "A", "B", "C")
head(x)

# Overview
?formula
# http://faculty.chicagobooth.edu/richard.hahn/teaching/FormulaNotation.pdf

# 1 intercept
# -1 exclude intercept
# The intercept is included by default in linear models, 
# but in other contexts you need to specify it.

lm(dv ~ A, x) # intercept included by default
lm(dv ~ 1 + A, x) # intercept explicitly included (same as above)
lm(dv ~ -1 + A, x) # exclude intercept

# + main effect
lm(dv ~ A + B, x) # main effect of A and B


# * include interaction and main effects
# : just main effect without interactions 
lm(dv ~ A * B, x) # main effects and interactions
lm(dv ~ A:B, x) # no main effects but interaction
lm(dv ~ A + B + A:B, x) # main effects explicitly specified
lm(dv ~ A*B*C, x) # main effects, two-way interactions, three-way interaction
lm(dv ~ (A + B + C)^3, x) # main as above
lm(dv ~ (A + B + C)^2, x) # main effects but only two-way interactions

# You can apply transformations to variables in place
lm(dv ~ scale(A), x) # main effects but only two-way interactions
# this is the same as creating a new variable
# and using he new variable in the model
x$zA <- scale(x$A)
lm(dv ~ zA, x)

# However if the transformation involves symbols that
# have special meaning in the context of R formulas
# i.e., +, -, *, ^, |, :
# then you  # have to wrap it in the I()
# I stands for Inhibit Interpretation or AsIs

# Polynomial regression
lm(dv ~ A + I(A^2), x) # include quadratic effect of A
lm(dv ~ A + I(A^2) + I(A^3), x) # include quadratic and cubic effect of A

# interaction effects with centering
lm(dv ~ A + B + I(scale(A) * scale(B)), x) # z-score centre before creating interaction

# composites
lm(dv ~ I(A + B), x) # include the sum of two variables as a predictor
lm(dv ~ I(2 * A + 5 * B), x) # include the weighted coposte as a predictor
```


# R Factors: Categorical predictors
```{r}
# Factors can be used for categorical variables

# http://www.ats.ucla.edu/stat/r/modules/factor_variables.htm
library(MASS)
data(survey)
csurvey <- na.omit(survey)
# let's assume a few variables were string variables
csurvey$Sex_character <- as.character(csurvey$Sex)
csurvey$Smoke_character <- as.character(csurvey$Smoke)

# by default character variables will be converted to factors in regression models
lm(Height ~ Sex_character, csurvey)
# by default it performs dummy coding with the first category as the reference category
# By default the ordering of a categorical variable is alphabetical

# levels shows the levels of a factor variable
# Thus, if we convert a sex as a character variable to a factor
# F is before M to it is Female then Male

csurvey$Sex_factor <-  factor(csurvey$Sex_character)
levels(csurvey$Sex_factor)
lm(Height ~ Sex_factor, csurvey)

# Factors also influence the ordering of categorical variables
# in plots
par(mfrow=c(2,1))
plot(Height ~ Sex_factor, csurvey) 
# and the order in tables
table(csurvey$Sex_factor)

# If we wanted to change the order to Male then Female
csurvey$Sex_factor <- factor(csurvey$Sex_character, levels = c("Male", "Female"))
levels(csurvey$Sex_factor)
lm(Height ~ Sex_factor, csurvey) # now male is the reference category
plot(Height ~ Sex_factor, csurvey) 
table(csurvey$Sex_factor)


# Ordered factors
# Factors  
# some factors reflect an ordinal relationship
# e.g., survey frequency-agreement type scales
# For example, see this smoking frequency items
csurvey$Smoke_factor <- factor(csurvey$Smoke)
table(csurvey$Smoke_factor)
# By default it is in the wrong order
csurvey$Smoke_factor <- factor(csurvey$Smoke, c("Never", "Occas", "Regul", "Heavy"))
table(csurvey$Smoke_factor)

# However, we can also influence the type of contrasts performed
csurvey$Smoke_ordered <- factor(csurvey$Smoke, c("Never", "Occas", "Regul", "Heavy"),
                                ordered = TRUE)
# or equivalently
csurvey$Smoke_ordered <- ordered(csurvey$Smoke, c("Never", "Occas", "Regul", "Heavy"))

# When included in linear model, we get
# polynomial contrasts for ordered factors
lm(Pulse ~ Smoke_ordered, csurvey)

# Many data import functions have the option of
# importing string variables as characters or factors
# Some use a general configuration option:
opt <- options()
opt$stringsAsFactors
# e.g., 
# read.table(..., stringsAsFactors = ...) 
# read.csv(..., stringsAsFactors = ...) 

# other functions have explicit options to import as factors
# foreign::read.spss(..., use.value.labels = ...

# Tip: My preference is to import string variables as character variables
# If I want to use factors I prefer to explicitly create them.
```

# Exercise 1
```{r exercise 1}
library(AER)
help(package = AER)
data("GSS7402")
?GSS7402 # to learn about the dataset
# It might be easier to work with a shorter variable name 

# 1. Run a t-test on whether participants who lived in a city
#    at age 16 (i.e, city16) have more or less education 
#    than those those who did not

# 2. Get correlations between education, number of kids (kids)
#    year, and number of siblings (siblings)

# 3. Run a multiple regresion predicting education from
#    year, kids, and siblings.
# 3.1 Run the model and save the fit

# 3.2 Get a summary of the results

# 3.3 the standardised coefficients

# 3.4 Check whether the residuals are normally distributed

# 3.5 Plot predicted values by residuals


# 4. Factors
# 4.1 create a table of values for ethnicity

# 4.2 Run a regression predicting education from ethnicity

# 4.3 Make a new factor variable where cauc is the reference value
#     and check that this worked by running a regression with 
#     this new ethncity variable as the predictor.


# 5. Comparing models
# 5.1 Fit a model predicting education from 
#     (a) year and siblings 
#     (b) year, siblings, and the interaction
#     and compare the fit of these two models

```

# Answers 1
```{r answers 1}
library(AER)
help(package = AER)
data("GSS7402")
?GSS7402 # to learn about the dataset
# It might be easier to work with a shorter variable name 
gss <- GSS7402

# 1. Run a t-test on whether participants who lived in a city
#    at age 16 (i.e, city16) have more or less education 
#    than those those who did not
t.test(education ~ city16, gss)

# 2. Get correlations between education, number of kids (kids)
#    year, and number of siblings (siblings)
cor( gss[ ,c("education", "kids", "year", "siblings")])


# 3. Run a multiple regresion predicting education from
#    year, kids, and siblings.
# 3.1 Run the model and save the fit
fit <- lm(education ~ year + kids + siblings, gss)

# 3.2 Get a summary of the results
summary(fit)

# 3.3 the standardised coefficients
QuantPsyc::lm.beta(fit)

# 3.4 Check whether the residuals are normally distributed
hist(residuals(fit))

# 3.5 Plot predicted values by residuals
plot(predict(fit), residuals(fit), pch =".")
par(mfrow = c(2, 2))
plot(fit, pch=".")
par(mfrow = c(1,1))


# 4. Factors
# 4.1 create a table of values for ethnicity
table(gss$ethnicity)

# 4.2 Run a regression predicting education from ethnicity
lm(education ~ ethnicity, gss)

# 4.3 Make a new factor variable where cauc is the reference value
#     and check that this worked by running a regression with 
#     this new ethncity variable as the predictor.
gss$ethnicity_other <- factor( gss$ethnicity, c("cauc", "other"))
lm(education ~ ethnicity_other, gss)


# 5. Comparing models
# 5.1 Fit a model predicting education from 
#     (a) year and siblings 
#     (b) year, siblings, and the interaction
# and compare the fit of these two models
fit1 <- lm(education ~ year + siblings, gss)
fit2 <- lm(education ~ year * siblings, gss)
summary(fit1)
summary(fit2)
anova(fit1, fit2)
```



# Illustration of how ideas generalise to other kinds of models
# Generalised linear models
```{r}
# Don't create median splits
# but for the sake of example assume that we have
# a binary outcome
cas$high_performance <- as.numeric(cas$performance > median(cas$performance))

# glm: generalised linear models
# E.g., logistic regression
fit <- glm(high_performance ~ calworks + lunch, cas, family = binomial())
summary(fit)
exp(coef(fit)) # exp beta coefficients
```

# Multilevel modelling
```{r}
# Main multilevel modelling package
library(lme4) 
# also see older package
# library(nlme)

# Let's look at the built-in sleepstudy dataset
data(sleepstudy)
?sleepstudy
# long format dat
head(sleepstudy, 20)

table(sleepstudy$Subject) # number of observations per participant
length(table(sleepstudy$Subject)) # number of participants
table(sleepstudy$Days) # each participants observed at times 0 to 9

# histogram of reaction time
hist(sleepstudy$Reaction, 10)

# Reaction time over days of sleep deprivation
# each cell is one subject
ggplot(sleepstudy, aes(x = Days, y = Reaction)) + 
    geom_point() +
    facet_wrap( ~ Subject)
    

# Random intercept
fit1 <- lmer(Reaction ~ 1 + (1  | Subject),  data = sleepstudy)

# Random intercept + fixed Days effect
fit2 <- lmer(Reaction ~ 1 + Days + (1  | Subject),  data=sleepstudy) 

# Random intercept and random Days effect
fit3 <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject),  data=sleepstudy) 

# # Random intercept and linear Days effect, fixed quadratic Days effect
fit4 <- lmer(Reaction ~ 1 + Days + I(Days^2) + (1 + Days | Subject),  data=sleepstudy) 

# Compare models
anova(fit1, fit2)
anova(fit2, fit3)
anova(fit3, fit4)

# Summary of best fitting model
summary(fit3)

# Most standard methods from lm also apply
plot(fit3) # plot fitted by residuals
hist(residuals(fit3)) # histogram of residuals

# Save and plot predicted values
sleepstudy$predicted_fit3 <- predict(fit3)
ggplot(sleepstudy, aes(x = Days, y = Reaction)) + 
    geom_point() + geom_line(aes(y=predicted_fit3))  +
    facet_wrap( ~ Subject)
```

# Exercise 2
```{r exercise 2}
# Let's create some simulated data with a random intercept
# and random slope.
set.seed <- 1234 # ensures we get the same results
sim <- expand.grid(subject = 1:20, time = 1:10)
sim_subject <- data.frame(subject = 1:20, 
                     intercept = rnorm(20, 0, 1),
                     beta = rnorm(20, .3, .2))
sim <- merge(sim, sim_subject)
sim$dv <- rnorm(nrow(sim),  sim$intercept + sim$beta * sim$time,  .6)

# 1. Plot the the effect of the dv by time over subjects

# 2. Fit models predicting dv from time by subject
#    (1) a random intercept model
#    (b) a random intercept plus fixed slope model
#    (c) a rndom intercept and random slope model

# 3. Get summary information for model 3

# Compare the fits of the three models
# which is best?
    
```

# Answers
```{r answers for exercise 2}
# Let's create some simulated data with a random intercept
# and random slope.
sset.seed <- 1234 # ensures we get the same results
sim <- expand.grid(subject = 1:20, time = 1:10)
sim_subject <- data.frame(subject = 1:20, 
                     intercept = rnorm(20, 0, 1),
                     beta = rnorm(20, .3, .2))
sim <- merge(sim, sim_subject)
sim$dv <- rnorm(nrow(sim),  sim$intercept + sim$beta * sim$time,  .6)

# 1. Plot the the effect of the dv by time over subjects
ggplot(sim, aes(x = time, y = dv)) + 
    geom_point() + facet_wrap( ~ subject)


# 2. Fit models predicting dv from time by subject
#    (1) a random intercept model
#    (b) a random intercept plus fixed slope model
#    (c) a rndom intercept and random slope model

fit1 <- lmer(dv ~ 1 + (1  | subject),  data = sim)
fit2 <- lmer(dv ~ 1 + time + (1  | subject),  data=sim) 
fit3 <- lmer(dv ~ 1 + time + (1 + time | subject),  data=sim) 

# 3. Get summary information for model 3
summary(fit3)

# Compare the fits of the three models
# which is best
anova(fit1, fit2)
anova(fit2, fit3) # model 3 is best
```


# Structural equation modelling
```{r}
# There are three main options for SEM
# library(sem): this is the original one
#
# library(OpenMx): Very powerful but more complicated
# http://openmx.psyc.virginia.edu/
#
# library(lavaan): 
# This is my first choice when it comes to doing
# all the standard things that you might do in a program like Amos 
# Lots of user friendly documentation on:
# http://lavaan.ugent.be/
# I also have a cheat sheet
# http://jeromyanglim.tumblr.com/post/33556941601/lavaan-cheat-sheet

library(lavaan)
library(psych)
data(bfi)

cbfi <- na.omit(bfi)

dim(cbfi)
head(cbfi)
dput(names(cbfi))
v$sem <- c("A1", "A2", "A3", "A4", "A5", "C1", "C2", "C3", "C4", "C5", 
    "E1", "E2", "E3", "E4", "E5", "N1", "N2", "N3", "N4", "N5", "O1", 
    "O2", "O3", "O4", "O5")

# Exploratory factor analysis
# Extract 5 factors with promax rotation 
psych::scree(cbfi[ v$sem]) # scree plot
fa <- factanal(cbfi[ v$sem], factors = 5, rotation = "promax") 
print(fa, cutoff=.3) # print results hiding loadings below .3

# Confirmatory factor analysis
# Write out SEM using model notation
model1 <- "
    # latent variable definitions
    # side point: first item gets loading of 1 so
    # it is clearer if this is a positively worded item
    agreeableness =~ A2 + A1 + A3 + A4 + 1 * A5
    conscientiousnes =~ C1 + C2 + C3 + C4 + C5
    extraversion =~ E3 + E1 + E2  + E4 + E5
    neuroticism =~ N1 + N2 + N3 + N4 + N5
    openness =~  O1 + O2 + O3 + O4 + O5
"

# fit model
fit1 <- cfa(model1, data=cbfi[ v$sem])
summary(fit1, fit.measures=TRUE)

# Suggest modifications
mod_ind <- modificationindices(fit1)
split(head(mod_ind[order(mod_ind$mi, decreasing=TRUE), ], 20), 
      head(mod_ind[order(mod_ind$mi, decreasing=TRUE), "op"], 20))


# Refine model
model2 <- "
    # latent variable definitions
    # side point: first item gets loading of 1 so
    # it is clearer if this is a positively worded item
    agreeableness =~ A2 + A1 + A3 + A4 + 1 * A5
    conscientiousnes =~ C1 + C2 + C3 + C4 + C5
    extraversion =~ E3 + E1 + E2  + E4 + E5
    neuroticism =~ N1 + N2 + N3 + N4 + N5
    openness =~  O1 + O2 + O3 + O4 + O5

    # add some correlated items that are very similar
    N1 ~~ N2
    N3 ~~ N4
    C1 ~~ C2
"

fit2 <- cfa(model2, data=cbfi[ v$sem])
summary(fit2, fit.measures=TRUE)

ff1 <- fitMeasures(fit1)
ff2 <- fitMeasures(fit2)
ff1

# show measures you want
dput(names(ff1))
v$stats <-  c("npar", "chisq", "df", "pvalue", 
   "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper")

# compare stats
round(data.frame(ff1[v$stats],  ff2[v$stats]), 3)
```


# Meta analysis
```{r}
# Lots of meta-analysis options
# http://cran.r-project.org/web/views/MetaAnalysis.html
# meta, rmeta, and metafor are all fairly general meta-analysis packages
library(metafor)

# Example is based on 
# http://www.metafor-project.org/doku.php/analyses:normand1999
data("dat.normand1999")
?dat.normand1999
# compares mean length of stay for stroke patients
# in speialised care (group 1) and routine care (group 2)
dat.normand1999
mean(dat.normand1999$m1i) # mean over studies length of time in specialised care
mean(dat.normand1999$m2i) # ...........                      in routine care

# calculate pooled standard deviation
dat.normand1999$sdpi <- with(dat.normand1999, 
                             sqrt(((n1i - 1) * sd1i^2 + (n2i - 1) * sd2i^2) /
                                      (n1i + n2i - 2)))
# Compare standard mean differences
dat <- escalc(m1i=m1i, sd1i=sdpi, n1i=n1i, m2i=m2i, sd2i=sdpi, n2i=n2i,
              measure="SMD", data=dat.normand1999, digits=2)
# Fit random effects meta analysis
fit <- rma(yi, vi, data=dat, method="HS", digits=2)
summary(fit) # Estimate of mean and sd of effect
forest(fit) # Plot of effect size estimates
```



# Bootstrapping
```{r}
library(boot)
# see also
# http://www.statmethods.net/advstats/bootstrapping.html


library(car)
# Use height and weight data of university students
data(Davis)
Davis <- na.omit(Davis)
Davis$bmi <- with(Davis, weight/(height/100)^2)
hist(Davis$bmi)
# looks like data entry error
Davis[ Davis$bmi > 100, ]
# let's remove and work with cleaned data
cdavis <-  Davis[ Davis$bmi < 100, ]

# Which correlation is larger
# Correlation between actual and report height 
# or correlation between actual and reported weight
par(mfrow=c(2,1))
plot(cdavis$height, cdavis$repht)
abline(a = 0, b = 1)
plot(cdavis$weight, cdavis$repwt)
abline(a = 0, b = 1)


# look at sample data
# correlation for weight looks a tiny bit bigger
# but is it significant
cor(cdavis$height, cdavis$repht)
cor(cdavis$weight, cdavis$repwt)

# How could we test this using a bootstrap?

# function receives
cordif <- function(data, i) {
    cidavis <- data[i, ]
    cor1 <- cor(cidavis$height, cidavis$repht)
    cor2 <- cor(cidavis$weight, cidavis$repwt)
    cor1 - cor2
}

fit <- boot(data = cdavis, statistic = cordif, R = 2000)
fit
boot.ci(fit)
```



# Bayesian modelling
```{r}
# See interfaces with Bayesian modelling language like
# library(rjags)  # JAGS
# and
# library(rstan) # Stan
#
# See example project:
# Anglim, J., & Wynton, S. K. (2015). Hierarchical Bayesian Models of 
# Subtask Learning. Journal of experimental psychology. Learning, memory, and cognition.
# Full repository with R code available at
# https://github.com/jeromyanglim/anglim-wynton-2014-subtasks
```

